{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921bb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, jit\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from smooth_binary_node import SmoothBinaryNode\n",
    "from utils import mse\n",
    "\n",
    "node = SmoothBinaryNode()\n",
    "\n",
    "# feature 1 will be used s criteria\n",
    "# The threshold is 50\n",
    "params = {'weights': jnp.array([0, 10.0, 0]),\n",
    "          'leaves': jnp.array([-1.0, 1.0]),\n",
    "          'bias': 50.0}\n",
    "features = jnp.array([[1, 43, 7], [1, 55, 7], [1, 46, 33],[1, 52, 16]])\n",
    "pred = node.predict(params, features)\n",
    "y_true = jnp.array([[-1, 1, -1, 1]])\n",
    "err = mse(node)\n",
    "\n",
    "# Compute gradient using jax\n",
    "jgrad = jit(grad(err))\n",
    "grads = jgrad(params, features, y_true)\n",
    "# Parameters are optimal\n",
    "# gradient is null\n",
    "pred = node.predict(params, features)\n",
    "print('pred (right pred)', pred)\n",
    "print('err (right pred):', err(params, features, y_true))\n",
    "print('grads (right pred)', grads)\n",
    "#> grads {'bias': Array(0., dtype=float32, weak_type=True), 'leaves': Array([0., 0.], dtype=float32), 'weights': Array([0., 0., 0.], dtype=float32)}\n",
    "\n",
    "# slightly perturbing leaves value\n",
    "params = {'weights': jnp.array([0, 10.0, 0]),\n",
    "          'leaves': jnp.array([-0.9, 1.0]),\n",
    "          'bias': 50.0}\n",
    "grads = jgrad(params, features, y_true)\n",
    "# Parameters are no more optimal\n",
    "# gradient is non null\n",
    "pred = node.predict(params, features)\n",
    "print('pred (leave err)', pred)\n",
    "print('err: (leave err)', err(params, features, y_true))\n",
    "print('grads (leave err)', grads)\n",
    "#> grads {'bias': Array(-1.1920918e-09, dtype=float32, weak_type=True), 'leaves': Array([0.20000005, 0.        ], dtype=float32), 'weights': Array([0., 0., 0.], dtype=float32)}\n",
    "# Error gradient indicates that the left leave value has to be increased\n",
    "\n",
    "\n",
    "# Let's try to perturbate bias this time\n",
    "params = {'weights': jnp.array([0, 10.0, 0]),\n",
    "          'leaves': jnp.array([-1.0, 1.0]),\n",
    "          'bias': 45.0}\n",
    "grads = jgrad(params, features, y_true)\n",
    "# Parameters are no more optimal\n",
    "# gradient is non null\n",
    "pred = node.predict(params, features)\n",
    "print('pred (bias err)', pred)\n",
    "print('err: (bias err)', err(params, features, y_true))\n",
    "print('grads', grads)\n",
    "#> grads {'bias': Array(-1.1920918e-09, dtype=float32, weak_type=True), 'leaves': Array([0.20000005, 0.        ], dtype=float32), 'weights': Array([0., 0., 0.], dtype=float32)}\n",
    "# Error gradient indicates that the left leave value has to be increased\n",
    "\n",
    "learning_rate = 0.1\n",
    "for i in range(0, 1000):\n",
    "    for param_name, param_grad in grads.items():\n",
    "        params[param_name] -= learning_rate * param_grad\n",
    "    grads = jgrad(params, features, y_true)\n",
    "\n",
    "pred = node.predict(params, features)\n",
    "print('params (trained)', params)\n",
    "print('pred (trained)', pred)\n",
    "print('err: (trained)', err(params, features, y_true))\n",
    "print('grads', grads)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
